---
title: "Methods using RTMB and Nimble in Fisheries"
subtitle: "Pacific Stock Assessment Renewal Workshop Series"
author: "Paul van Dam-Bates"
institute: ""
date: "September 2024"
output:
  xaringan::moon_reader:
    css: ["xaringan-themer.css", "theme.css"]
    lib_dir: libs
    nature:
      highlightLines: true
      countIncrementalSlides: false
---

<!-- Build with: xaringan::inf_mr() -->

```{r preamble, include=FALSE, cache=FALSE}
xaringanthemer::style_mono_accent(
  base_color = "#202020",
  header_font_google = xaringanthemer::google_font("Raleway"),
  text_font_google = xaringanthemer::google_font("Open Sans"),
  code_font_google = xaringanthemer::google_font("Fira Mono"),
  title_slide_background_size = "14%",
  title_slide_background_position = "50% 90%",
  base_font_size = "20px",
  header_h1_font_size = "2.1rem",
  text_font_size = "1.5rem",
  code_font_size = "1.1rem",
  link_color = "#0047AB"
)
knitr_opts <- list(
  message = FALSE,
  warning = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  dpi = 300,
  out.width = "700px",
  fig.asp = 1 / 1.618,
  cache = TRUE,
  autodep = TRUE,
  cache.comments = TRUE,
  fig.align = "center",
  echo = FALSE,
  results = 'hide'
)
do.call(knitr::opts_chunk$set, knitr_opts)
```

```{r libs, include=FALSE}
library(ggplot2)
library(dplyr)
library(RTMB)
options(ggplot2.continuous.colour = "viridis")
options(ggplot2.continuous.fill = "viridis")
ggplot2::theme_set(ggplot2::theme_minimal())
```

# Part I: Introduction to RTMB

---

# Basic Example

```{r, echo = TRUE}
data(ChickWeight)
```

Consider chick weight data where $w$ = weight, $t$ = time, $k$ = chick, and $D$ = diet. 

$$w_i = \beta_0 + \beta_t t_i + \beta_{D_i}+ \epsilon_i.$$

In this case, 

.xsmall[
$$\epsilon_i \sim \text{normal}(\mu_i, \sigma^2),$$
]

where 

.xsmall[
$$\mu_i = \beta_0 + \beta_t t_i + \beta_{D_i}.$$
]

---

# Coding a factor

1. Levels 2-4 are coded as level 1 (Intercept) + difference.

```{r, echo = TRUE}
  Design1 <- matrix(0, nrow = nrow(ChickWeight), ncol = 4)
  Design1[cbind(1:nrow(ChickWeight), ChickWeight$Diet)] <- 1
  Design1[,1] <- 1  ## All observations get Intercept.
  Design1 <- cbind(Design1, ChickWeight$Time)
```

2. Each diet gets a different Intercept.

```{r, echo = TRUE}
  Design2 <- matrix(0, nrow = nrow(ChickWeight), ncol = 4)
  Design2[cbind(1:nrow(ChickWeight), ChickWeight$Diet)] <- 1
  Design2 <- cbind(Design2, ChickWeight$Time)
```

---

# Constructing a log-likelihood

- The probability density function for an observed weight is

.xsmall[
$$
  f(w_i|\mathbf{\theta}) = \text{normal}(\mu_i, \sigma^2)
$$
]

- The likelihood is then the product of the densities, $f(\cdot)$, over all $n$ observations,

.xsmall[
$$
L(\mathbf{\theta}|\mathbf{w}) = \prod_{i=1}^n f(w_i|\mathbf{\theta}).
$$
]

---

# Constructing a log-likelihood

- We work on the log-scale as probabilities become small and tend to underflow, creating numerical issues. This leads to the sum of the log of each observed density $f$,

$$
l(\mathbf{\theta}|\mathbf{w}) = log(L(\mathbf{\theta}|\mathbf{w})) = \sum_{i=1}^n log(f(w_i|\mathbf{\theta})).
$$

- Functions like `optim` or `nlminb` that do optimization in R seek to MINIMIZE the function as a default. For maximum likelihood estimation, we want to find the maximum of $l(\mathbf{\theta}|\mathbf{w})$, or the minimum of $-l(\mathbf{\theta}|\mathbf{w})$.

---

# Code the negative log-likelihood

```{r, echo = TRUE}
  ## Parameters are coefficients in the design matrix and variance sigma:
  fn <- function(par){
    beta <- par[1:ncol(X)]  ## X is Design Matrix
    sigma <- exp(par[ncol(X)+1])  ## Is this necessary?
    ## Do yourself...
  }
```

```{r, echo = FALSE}
  ## Parameters are coefficients in the design matrix and variance sigma:
  fn <- function(par){
    beta <- par[1:ncol(Design1)]  ## X is Design Matrix
    sigma <- exp(par[ncol(Design1)+1])
    mu <- Design1 %*% beta
    sum(-dnorm(ChickWeight$weight, mu, sigma, log = TRUE))
  }
```
```{r, echo = TRUE, results = "markup"}
fit <- lm(weight ~ Diet + Time, data = ChickWeight)
fit2 <- nlminb(c(rep(0, ncol(Design1)),0), fn)
coef(fit) - fit2$par[1:ncol(Design1)]
sigma(fit) - exp(fit2$par[ncol(Design1)+1])
```

---

# Why RTMB

- Speed - Makes R run in C++

```{r, echo = TRUE, results="markup"}
  library(RTMB)
  pars <-  c(rep(0, ncol(Design1)),0)
  obj <- MakeADFun(fn, pars, silent = TRUE)
  microbenchmark::microbenchmark(
      rtmb = obj$fn(pars),
      baseR = fn(pars))
```

---

# Why RTMB

- Automatic Differentiation - Accurate!

```{r}
gr_true <- function(par){
  beta <- par[1:ncol(Design1)]  ## X is Design Matrix
  sigma <- exp(par[ncol(Design1)+1])
  mu <- Design1 %*% beta
  gr <- rep(0, length(par))
  for( i in 1:length(beta)){
      gr[i] <- sum(-1/sigma^2*(ChickWeight$weight-mu)*Design1[,i])
  }
  gr[i+1] <- -nrow(Design1) + sum((ChickWeight$weight-mu)^2*sigma^-2)
  return(gr)
}
gr_true(fit2$par)
```

```{r, echo = TRUE, results="markup"}
## Finite Difference vs AD
gr_true(fit2$par) - pracma::jacobian(obj$fn, fit2$par)
gr_true(fit2$par) - obj$gr(fit2$par)
```

---

# Why RTMB

- Automatic Differentiation - Accurate!
- Automatic Differentiation - Fast!

```{r, echo = TRUE, results="markup"}
microbenchmark::microbenchmark(
      rtmb = obj$gr(fit2$par),
      baseR = pracma::jacobian(obj$fn, fit2$par))

```

---

# Why do derviatives matter?

1. Helps us find the maximum/minimum, when the gradient (vector of partial derivatives) is zero.
1. The variance is defined by the inverse of the Hessian (of the negative log likelihood). Hessian is the derivative of the derivative.
1. Finite difference methods are very slow and often inaccurate.
1. Automatic differentiation can be faster than analytically defined gradients due to the building of a tape. The tape is an efficient set of rules for a chain rule where a lot of the algebra is pre-computed as constants (when building the tape).
---

# The Laplace Approximation

- The Laplace approximation approximately integrates "normal looking" likelihood surfaces by a single point evaluation at the mode.
- Consider the above Chick Weights problem with a random effect per chick.

```{r, echo = TRUE, results="markup"}
  library(glmmTMB)
  fit.glmm <- glmmTMB(weight ~ Diet + Time + (1|Chick), data = ChickWeight)
  fixef(fit.glmm)
```
---

# New Log-Likelihood

- Each random-effect, $u_k$ is assumed to be normally distributed with mean 0, and variance $\sigma_{re}^2$.

- The contribution for the random-effects for $N$ chicks is then,

.xsmall[
$$
f(\mathbf{u}|\boldsymbol{\theta}) = \prod_{k=1}^N f(u_k|\boldsymbol{\theta})
$$
]

- The change in the log-likelihood happens to the expected value of each observation,

$$\mu_i = \beta_0 + \beta_t t_i + \beta_{D_i} + u_{chick_i}$$


---

# New Log-Likelihood

- The new likelihood is then

.xsmall[
$$L(\boldsymbol{\theta}|\mathbf{w},\mathbf{u}) = f(\mathbf{u}|\boldsymbol{\theta}) \prod_{i=1}^n f(w_i|u_{chick_i}, \boldsymbol{\theta}).$$
]

- The log-likelihood is defined as the sum over the log observed densities plus, the sum of the random effect densities,

$$l(\boldsymbol{\theta}|\mathbf{w},\mathbf{u}) = \sum_{k=1}^N log(f(u_i|\boldsymbol{\theta})) + \sum_{k=i}^n log(f(w_i|u_{chick_i}, \boldsymbol{\theta})).$$

---

```{r, echo = TRUE}
  ChickWeight$chick <- as.integer(as.character(ChickWeight$Chick))
  pars.re <- list(beta = rep(0, ncol(Design1)),
                  logsigma = 0, logsigmare = 0,
                  re = rep(0, max(ChickWeight$chick)))
  ## Parameters are coefficients in the design matrix and variance sigma:
  fn <- function(par){
    getAll(par)
    sigma <- exp(logsigma)
    sigmare <- exp(logsigmare)
    mu <- Design1 %*% beta + re[as.numeric(ChickWeight$Chick)]
    ADREPORT(sigma)
    ADREPORT(sigmare)
    negll <- sum(-dnorm(ChickWeight$weight, mu, sigma, log = TRUE)) 
    negll <- negll - sum(dnorm(re, 0, sigmare, log = TRUE))
    negll
  }
  obj <- MakeADFun(fn, parameters=pars.re, silent = TRUE)
  fit.re <- nlminb(obj$par, obj$fn, obj$gr)
  sdreport(obj)
```

---

# Laplace Continued

- To actually make inference we need to marginalize out the $N$ unobserved random effects, $\mathbf{u}$.

.xsmall[
$$f(\mathbf{w}|\mathbf{\theta}) = \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty f(\mathbf{w}| \mathbf{u}, \mathbf{\theta}) f(\mathbf{u} | \mathbf{\theta}) du_{1} \cdots d_{N}.$$
]

- This is an N dimensional integral, which in this particular case, can be written as $N$ single dimension integrals as each chick is independent.

- Works well when this likelihood is unimodal (has one maximum) and looks roughly normal locally. Actually EXACT in the example.

---

# Laplace Continued

- The Laplace approximation works by creating a Normal distribution that looks kind of like the posterior distribution of the random-effects we are integrating out,
.xsmall[
$$f_G(\mathbf{u}|\mathbf{w}, \mathbf{\theta}) \approx \text{Multivariate-Normal}(\widehat{\mathbf{u}}, H_{\widehat{u}}^{-1}).$$
]

- Then, evaluated at the mean,

.xsmall[
$$f_G(\widehat{\mathbf{u}} | \mathbf{w}, \mathbf{\theta}) = \frac{det(H_\widehat{u})^{0.5}}{(2\pi)^{N/2}}$$
]

- The Laplace approximation is then,

.xsmall[
$$f(\mathbf{w}|\mathbf{\theta}) \approx  f(\mathbf{w}| \mathbf{u}, \mathbf{\theta}) f(\mathbf{u} | \mathbf{\theta})/ f_G(\widehat{\mathbf{u}} | \mathbf{w}, \mathbf{\theta})$$
]

---

# Laplace Continued

- The recipe is for Laplace is then to find $\widehat{\mathbf{u}}$ for some value of $\boldsymbol{\theta}$, and then find the Hessian $H_{\widehat{u}}$ and calculate,

$$l(\boldsymbol{\theta}|\mathbf{w}) \approx \frac{N}{2}log(2\pi) - log(det(H_{\widehat{u}})) + l(\boldsymbol{\theta}|\mathbf{w},\widehat{\mathbf{u}})$$

- Let's do it in RTMB now manually.

---

# RTMB Laplace

```{r, echo = TRUE}
  nre <- max(ChickWeight$chick)
  obj.re <- MakeADFun(fn, parameters = pars.re, silent = TRUE, 
      map = list(beta = factor(rep(NA, ncol(Design1))), 
        logsigma = factor(NA), 
        logsigmare = factor(NA)))
  fit.re.only <- nlminb(obj.re$par, obj.re$fn, obj.re$gr)
  
  laplace_approx <- -nre*log(2*pi) + 
      log(det(obj.re$he(fit.re.only$par))) +   obj.re$fn(fit.re.only$par)
  
  obj.laplace <- MakeADFun(fn, parameters = pars.re, 
                            silent = TRUE, random = "re")
  obj.laplace$fn(obj.laplace$par) - laplace_approx
  obj.laplace$env$last.par[-(1:(ncol(Design1)+2))] - fit.re.only$par
```

---

# RTMB Laplace

```{r, echo = TRUE, show = 'markup'}

  fit.laplace <- nlminb(obj.laplace$par, obj.laplace$fn, obj.laplace$gr)
  fit.laplace$par[1:ncol(Design1)] - fixef(fit.glmm)[[1]] ## Matched it.
```

---

# Exercises

Choose one of:

1. Write a general RTMB function that does Laplace manually.
1. Find an example where you don't think Laplace will work well.

---

# Part II: Stock-Recruit Example

---

# Ricker Curve

- Consider a Ricker stock recruit relationship for salmon, where $R_t$ is recruitment from year $t$ spawners $S_t$,

$$R_t = \alpha S_t e^{-\beta S_t}$$
- As a linear equation,
$$log(R_t/S_t) = log(\alpha) - \beta S_t$$

- Install `samEst` from https://github.com/Pacific-salmon-assess/samEst/tree/main
- We will use `data(harck)` from the samest package for this section.

---

# The Basic Linear Model

- Code a linear model for the Harrison Chinook stock-recruitment data using RTMB (assuming Gaussian error structure for log(R/S)).
- Can you get the same results as `glmmTMB`?

```{r, echo = TRUE, show= 'markup'} 
  library(samEst)
  data(harck)
  fit.harck <- glmmTMB(logRS ~ S, data=harck)
  summary(fit.harck)
```

---

## One Version

```{r, echo = TRUE}
  fn <- function(par){
    getAll(par, harck)
    beta <- exp(logbeta)
    sigma <- exp(logsigma)

    Expected_logRS <- logalpha - beta*S
    negll <- -sum(dnorm(logRS, mean = Expected_logRS, sd = sigma, log = TRUE))
    ADREPORT(sigma)
    alpha <- exp(logalpha)
    ADREPORT(alpha)
    ADREPORT(beta)
    return(negll)
  }

  pars <- list(logalpha = 0, logbeta = 0, logsigma = 0)
  obj <- MakeADFun(fn, pars, silent = TRUE)
  fit <- nlminb(obj$par, obj$fn, obj$gr)
  sdrep <- sdreport(obj)  ## All the values
  summary(sdrep, "fixed", p.value = TRUE)  
  summary(sdrep, "report", p.value = TRUE)
```
---

# Understanding Variance

- Variance for the parameters being optimized can be written based on the Hessian,

.xsmall[
$$V(\boldsymbol{\theta}) = H_\theta^{-1}.$$
]

- For a single term, $V(log(\alpha)) = V(\boldsymbol{\theta})_{1,1}$.
- Example,

```{r, echo = TRUE, show = 'markup'}
H <- obj$he(fit$par)
V <- solve(H)
sqrt(diag(V)) - summary(sdrep, "fixed")[,2]
```
---

# Understanding Variance

- What about the transformed variables? RTMB uses the Delta method,

.xsmall[
$$V(g(\theta)) \approx \nabla g V(\theta) \nabla g'$$
]

```{r, echo = TRUE, show = 'markup'}
  grad <- exp(fit$par)
  Vg <- t(grad %*% V) %*% grad
  sqrt(diag(Vg)) - summary(sdrep, "report")[c("alpha", "beta", "sigma"),2]
```
- By now it should be really clear that if we can write fast code that efficiently returns derivatives, inference is pretty easy.

---

# Excersizes

- Write an auto regressive process for the `harck` data in RTMB.
- Calculate and generate confidence intervals for maximum sustainable yield ( $S_{msy}$ ) and for the number of spawners needed to return to $S_{msy}$ in one generation, $S_{gen}$ .


- Example: https://github.com/Pacific-salmon-assess/samEst/blob/main/R/LamW_RTMB.r

```{r}
FritschIter <- function(x, w){
  MaxEval <- 5
  CONVERGED <- FALSE
  k <- 2.0 / 3.0;
  i <- 0;
  eps <- 2.2204460492503131e-16    
  while (!CONVERGED & i < MaxEval){
    z <- log(x / w) - w
    w1 <- w + 1.0
    q <- 2.0 * w1 * (w1 + k * z)
    qmz <- q - z
    e <- z / w1 * qmz / (qmz - z)
    CONVERGED <- abs(e) <= eps
    w <- w*(1.0 + e)
    i <- i + 1
  }
  return(w)
}
LambertW0_internal <- function(x){
  check <- 0.367879441171442334024277442949824035167694091796875 # exp(-1)
  eps <- 2.2204460492503131e-16
  if (x == Inf) {
    return(Inf);
  } else if (x < -check) {
    return(NaN);
  } else if (abs(x - check) <= eps) {
    return(-1.0);
  } else if (abs(x) <= 1e-16) {
    ## This close to 0 the W_0 branch is best estimated by its Taylor/Pade
    ## expansion whose first term is the value x and remaining terms are below
    ## machine double precision. See
    ## https://math.stackexchange.com/questions/1700919

    return(x);
  } else {
    w <- 0
    if (abs(x) <= 6.4e-3) {
      ## When this close to 0 the Fritsch iteration may underflow. Instead,
      ## function will use degree-6 minimax polynomial approximation of Halley
      ## iteration-based values. Should be more accurate by three orders of
      ## magnitude than Fritsch's equation (5) in this range.
      return((((((-1.0805085529250425e1 * x + 5.2100070265741278) * x -
             2.6666665063383532) * x + 1.4999999657268301) * x -
             1.0000000000016802) * x + 1.0000000000001752) * x +
             2.6020852139652106e-18);

    } else if (x <= exp(1)) {
      ## Use expansion in Corliss 4.22 to create (2, 2) Pade approximant.
      ## Equation with a few extra terms is:
      ## -1 + p - 1/3p^2 + 11/72p^3 - 43/540p^4 + 689453/8398080p^4 - O(p^5)
      ## This is just used to estimate a good starting point for the Fritsch
      ## iteration process itself.
      
      p <- sqrt(2.0 * (exp(1) * x + 1.0))
      Numer <- (0.2787037037037037 * p + 0.311111111111111) * p - 1.0;
      Denom <- (0.0768518518518518 * p + 0.688888888888889) * p + 1.0;
      w <- Numer / Denom;
    } else {
      ## Use first five terms of Corliss et al. 4.19 */
      w <- log(x)
      L_2 <- log(w)
      L_3 <- L_2 / w
      L_3_sq <- L_3 * L_3
      w <- w - L_2 + L_3 + 0.5 * L_3_sq - L_3 / w + L_3 / (w * w) - 1.5 * L_3_sq /
        w + L_3_sq * L_3 / 3.0;
    }
    return(FritschIter(x, w));
  }
}

dLambertW0_internal <- function(x, y, dy) {
  dy / (exp(y) * (1. + y))
}

LambertW0 <- RTMB:::ADjoint(LambertW0_internal, dLambertW0_internal)
```

---

# RTMB Tricks - ADJoint

- If you know the derivative then you can use it instead of AD.

```{r, echo = TRUE}
x <- rnorm(100, 0.25, 0.1)
myneg_dnorm <- function(theta){
  sum(-dnorm(x, theta[1], theta[2], log = TRUE))
}
gr_myneg_dnorm <- function(theta, f, df) {
  deriv <- numeric(2)
  deriv[1] <- -sum(x-theta[1])/theta[2]^2
  deriv[2] <- length(x)/theta[2] - sum((x-theta[1])^2)/theta[2]^3
  df * deriv ## Not sure if I need the df...
}
test <- ADjoint(myneg_dnorm, gr_myneg_dnorm )
grtest <- MakeTape(test, c(0,1))
fad <- MakeTape(myneg_dnorm, c(0,1))
fad$jacobian(c(0,1)) - grtest$jacobian(c(0,1))
```


---

# RTMB Tricks - Update Data

- Update data without running MakeADFun again.

```{r, echo = TRUE}
f <- function(p) {
    getDat <- function(x) {
        .GlobalEnv$mydat
    }
    empty <- advector(rep(0, 0)) ## Important this is empty but ad variable...
    ## Add R function to the AD tape
    y <- DataEval(getDat, empty)
    -sum(dnorm(y, p$mu, p$sd, log=TRUE))
}
mydat <- rnorm(10000, 0, 10)
obj <- MakeADFun(f, list(mu=0, sd=1), silent = TRUE)
fit1 <- nlminb(obj$par, obj$fn, obj$gr)
mydat <- rnorm(10000, 10, 1)
fit2 <- nlminb(obj$par, obj$fn, obj$gr)
fit1$par
fit2$par
```

---

# Now for Nimble...
